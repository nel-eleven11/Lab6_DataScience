{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 6\n",
    "## Data Science\n",
    "\n",
    "Autores:\n",
    "\n",
    "- Nelson García \n",
    "- Christian Echeverría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO4Yvxr1PbBT"
   },
   "source": [
    "## Análisis de Sentimientos de críticas de películas\n",
    "\n",
    "Junto con Keras, viene un ejemplo imdb_lstm.py. Este ejercicio esta prácticamente basado en él.\n",
    "\n",
    "Es un gran ejemplo del uso de las RNNs.  El conjunto de datos que se utilizará consta de críticas de películas generadas por usuarios, y una classificación indicando si le gustó, o no, basado en su rating asociado. \n",
    "\n",
    "Hay más información de este conjunto de datos en:\n",
    "\n",
    "https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\n",
    "\n",
    "Como la comprensión del lenguaje escrito requiere \"llevar cuenta\" de todas las palabras en una oración, necesitamos una RNN para mantener una \"memoria\" de las palabras que pasaron antes, conforme va \"leyendo\" oraciones a lo largo del tiempo. \n",
    "\n",
    "En particular, se usarán unidades LSTM (Long Short-Term Memory) porque no es deseable \"olvidar\" palabras demasiado rápido...las palabras al inicio de una oración pueden afectar el significado de la misma grandemente.\n",
    "\n",
    "Empezamos por la importación de lo que se requiere:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"   # habilita growth desde el arranque\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-5GymP2ePbBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.20.0\n",
      "Build info: OrderedDict({'cpu_compiler': 'clang 18', 'cuda_compute_capabilities': ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90'], 'cuda_version': '12.5.1', 'cudnn_version': '9', 'is_cuda_build': True, 'is_rocm_build': False, 'is_tensorrt_build': False})\n",
      "GPUs detectadas: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"Build info:\", getattr(tf.sysconfig, \"get_build_info\", lambda: {})())\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs detectadas:\", gpus)\n",
    "\n",
    "# Activa growth explícitamente (ya debería estar por env var, pero así confirmas)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers, regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aFJnFb7PbBd"
   },
   "source": [
    "Ahora importar los datos para entrenamiento y prueba.  Para que sea más manejable, se especifica que se quieren solamente las 50,000 palabras más populares en el conjunto de datos. Por algún motivo, este conjunto tiene una relación de 50% entreno y 50% prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ThMIo9DYPbBd",
    "outputId": "0dbe0c33-c2b7-427c-e4ac-fc97e80cee85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando los datos...\n"
     ]
    }
   ],
   "source": [
    "print('Cargando los datos...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_EZvv_6PbBd"
   },
   "source": [
    "A ver cómo son los datos, el primer elemento de entrenamiento debe ser una crítica de una película:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPsneaWNPbBe",
    "outputId": "cc84e444-4b55-4a9f-f3db-068d91ec85c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoHWVuCEPbBe"
   },
   "source": [
    "Esto no parece una crítica de una película!!!!  Resulta que la gente que preparó los datos ya hizo algo de preparación previa de los datos.  Estos números coinciden con el índice correspondiente a cada palabra de la crítica.  En realidad las palabras en sí, no son de interés...el modelo requiere números no palabras. \n",
    "\n",
    "Lo triste es que no será posible leer las críticas...siquiera para tener una idea de si funciona el análiisis, o no.\n",
    "\n",
    "Y, ¿cómo son las etiquetas (metas)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxKy_iFPPbBe",
    "outputId": "7fd0922e-9098-4b26-c8d2-078a296c1074"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPLmsWk5PbBe"
   },
   "source": [
    "Son simplemente 0 ó 1, que indica sí al que escribió la crítica le gustó, o no, la película.\n",
    "\n",
    "Para resumir, para el entrenamiento se tiene un conjunto de críticas de películas que han sido convertidas a vectores de palabras representadas por enteros, y una clasificación de sentimiento binaria.\n",
    "\n",
    "Las RNNs pueden \"explotar\" muy rápidamente (se habló de esto en clase).  Para que no se sobrecarguen las PCs que se podrían usar, se limitarán las críticas a las primeras 80 palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "td7nNbGcPbBf"
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen = 80)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de features adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# Inicializar el analizador de sentimientos\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Función para obtener características adicionales\n",
    "def extract_features(texts, max_length=80):\n",
    "    features = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Convertir el índice a palabras\n",
    "        word_index = imdb.get_word_index()\n",
    "        index_word = {v: k for k, v in word_index.items()}\n",
    "        words = [index_word.get(i - 3, '') for i in text]  # Descontamos 3 por los valores reservados (pad, start, unknown)\n",
    "\n",
    "        # 1. Longitud de la crítica\n",
    "        features.append([len(words)])\n",
    "        \n",
    "        # 2. Proporción de palabras positivas/negativas\n",
    "        pos_words, neg_words = 0, 0\n",
    "        for word in words:\n",
    "            if word:\n",
    "                sentiment = sia.polarity_scores(word)\n",
    "                if sentiment['compound'] > 0:\n",
    "                    pos_words += 1\n",
    "                elif sentiment['compound'] < 0:\n",
    "                    neg_words += 1\n",
    "        total_words = len([word for word in words if word])  # Palabras no vacías\n",
    "        pos_neg_ratio = pos_words / neg_words if neg_words > 0 else pos_words\n",
    "        \n",
    "        features[-1].extend([pos_words, neg_words, pos_neg_ratio])\n",
    "        \n",
    "        # 3. Número de palabras únicas\n",
    "        unique_words = len(set(words))\n",
    "        features[-1].append(unique_words)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Extraer características de entrenamiento y prueba\n",
    "train_features = extract_features(X_train)\n",
    "test_features = extract_features(X_test)\n",
    "\n",
    "# Concatenar características con las etiquetas de salida (y_train y y_test)\n",
    "x_train_features = np.hstack((train_features, X_train))\n",
    "x_test_features = np.hstack((test_features, X_test))\n",
    "\n",
    "# Ahora x_train_features y x_test_features tienen las características adicionales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUFGeOFOPbBf"
   },
   "source": [
    "## Configuración del modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "maxlen = int(X_train.shape[1])\n",
    "n_features = int(train_features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TrgEmhr2PbBf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757570928.793712    5594 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1728 MB memory:  -> device: 0, name: NVIDIA GeForce MX150, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "modelo = Sequential()\n",
    "modelo.add(Embedding(50000, 128))\n",
    "modelo.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelo.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RGjVEFQxPbBg"
   },
   "outputs": [],
   "source": [
    "modelo.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de features adicionales dentro del grafo (evita fugas de datos)\n",
    "normalizador = layers.Normalization(axis=-1, name=\"normalizador_features\")\n",
    "normalizador.adapt(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# MODELO 1: LSTM básico + rama densa para features (simple)\n",
    "# ======================================================\n",
    "# Entrada 1: secuencia de palabras (tokens)\n",
    "inp_seq_1 = layers.Input(shape=(maxlen,), name=\"input_secuencia\")\n",
    "\n",
    "# Rama de texto\n",
    "x1 = layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True, name=\"embedding\")(inp_seq_1)\n",
    "x1 = layers.LSTM(64, name=\"lstm_64\", use_cudnn=False)(x1)  # representación secuencial\n",
    "\n",
    "# Entrada 2: features adicionales (numéricas)\n",
    "inp_feat_1 = layers.Input(shape=(n_features,), name=\"input_features\")\n",
    "f1 = normalizador(inp_feat_1)\n",
    "f1 = layers.Dense(32, activation=\"relu\", name=\"feat_dense_32\")(f1)\n",
    "\n",
    "# Fusión de ramas\n",
    "h1 = layers.Concatenate(name=\"concat\")([x1, f1])\n",
    "h1 = layers.Dense(64, activation=\"relu\", name=\"post_concat_dense_64\")(h1)\n",
    "h1 = layers.Dropout(0.5, name=\"dropout_05\")(h1)\n",
    "\n",
    "# Capa de salida\n",
    "out_1 = layers.Dense(1, activation=\"sigmoid\", name=\"salida\")(h1)\n",
    "\n",
    "modelo_basico = keras.Model(inputs=[inp_seq_1, inp_feat_1], outputs=out_1, name=\"modelo_lstm_basico\")\n",
    "\n",
    "modelo_basico.compile(\n",
    "    optimizer=optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.AUC(name=\"auc\"), \"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo mejorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_right_padding(arr, pad_value=0):\n",
    "    arr = np.asarray(arr)\n",
    "    out = np.full_like(arr, pad_value)\n",
    "    for i in range(arr.shape[0]):\n",
    "        row = arr[i]\n",
    "        nz = row[row != pad_value]  # elimina ceros de pad\n",
    "        L = nz.shape[0]\n",
    "        out[i, :L] = nz             # tokens al inicio, pad al final\n",
    "    return out\n",
    "\n",
    "X_train2, X_test2 = X_train, X_test\n",
    "\n",
    "X_train_ = to_right_padding(X_train2, pad_value=0)\n",
    "X_test_  = to_right_padding(X_test2,  pad_value=0)\n",
    "\n",
    "maxlen = int(X_train_.shape[1])\n",
    "n_features = int(train_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# MODELO 2: Arquitectura avanzada (BiLSTM apiladas + regularización)\n",
    "# - SpatialDropout1D para robustez\n",
    "# - BiLSTM con return_sequences + BiLSTM final\n",
    "# - MLP más profundo en rama de features\n",
    "# - Regularización L2 y Dropout\n",
    "# ======================================================\n",
    "inp_seq_2 = layers.Input(shape=(maxlen,), name=\"input_secuencia_avanzado\")\n",
    "\n",
    "# Rama de texto avanzada\n",
    "x2 = layers.Embedding(input_dim=vocab_size, output_dim=256, mask_zero=True, name=\"embedding_avanzado\")(inp_seq_2)\n",
    "x2 = layers.SpatialDropout1D(0.2, name=\"spatial_dropout\")(x2)\n",
    "x2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2,implementation=2), name=\"bilstm_128_rs\")(x2)\n",
    "x2 = layers.Dropout(0.3, name=\"dropout_seq_03\")(x2)\n",
    "x2 = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2,implementation=2), name=\"bilstm_64\")(x2)\n",
    "x2 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4), name=\"text_dense_64\")(x2)\n",
    "\n",
    "# Rama de features avanzada\n",
    "inp_feat_2 = layers.Input(shape=(n_features,), name=\"input_features_avanzado\")\n",
    "f2 = normalizador(inp_feat_2)\n",
    "f2 = layers.Dense(64, activation=\"relu\", name=\"feat_dense_64\")(f2)\n",
    "f2 = layers.Dropout(0.3, name=\"feat_dropout_03\")(f2)\n",
    "f2 = layers.Dense(32, activation=\"relu\", name=\"feat_dense_32_b\")(f2)\n",
    "\n",
    "# Fusión\n",
    "h2 = layers.Concatenate(name=\"concat_avanzado\")([x2, f2])\n",
    "h2 = layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4), name=\"fusion_dense_128\")(h2)\n",
    "h2 = layers.Dropout(0.5, name=\"fusion_dropout_05\")(h2)\n",
    "h2 = layers.Dense(64, activation=\"relu\", name=\"fusion_dense_64\")(h2)\n",
    "\n",
    "out_2 = layers.Dense(1, activation=\"sigmoid\", name=\"salida_avanzada\")(h2)\n",
    "\n",
    "modelo_avanzado = keras.Model(inputs=[inp_seq_2, inp_feat_2], outputs=out_2, name=\"modelo_lstm_avanzado\")\n",
    "\n",
    "modelo_avanzado.compile(\n",
    "    optimizer=optimizers.Adam(3e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[keras.metrics.AUC(name=\"auc\"), \"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gWoZ4vUPbBg"
   },
   "source": [
    "## Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar detención temprana para monitorear el entrenamiento\n",
    "detencion_temprana = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIqVu_VEPbBh",
    "outputId": "31808fcc-3698-4863-e3dc-c4ddfb080be6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 282ms/step - accuracy: 0.7831 - loss: 0.4604 - val_accuracy: 0.8339 - val_loss: 0.3799\n",
      "Epoch 2/15\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 318ms/step - accuracy: 0.8868 - loss: 0.2841 - val_accuracy: 0.8238 - val_loss: 0.3923\n",
      "Epoch 3/15\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 326ms/step - accuracy: 0.9288 - loss: 0.1855 - val_accuracy: 0.8293 - val_loss: 0.4398\n",
      "Epoch 4/15\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 330ms/step - accuracy: 0.9534 - loss: 0.1303 - val_accuracy: 0.8242 - val_loss: 0.4736\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "historia = modelo.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    batch_size = 64,  # Puede ajustarse de acuerdo a la memoria de GPU disponible \n",
    "    epochs = 15,\n",
    "    verbose = 1,     \n",
    "    validation_data = (X_test, y_test),\n",
    "    callbacks = [detencion_temprana]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "# Después del entrenamiento se puede revisar el historial\n",
    "print(historia.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8ip7E_-UPbBi"
   },
   "outputs": [],
   "source": [
    "modelo.save(\"Analisis_Sentimiento.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mBG9S2-JPbBi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "modelo = load_model(\"Analisis_Sentimiento.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, ahora a evaluar la exactitud del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLehygHpPbBi",
    "outputId": "22f98634-7fa3-4c1c-f709-9247286572dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 - 16s - 42ms/step - accuracy: 0.8242 - loss: 0.4736\n",
      "Pérdida de la Prueba: 0.473602294921875\n",
      "Exactitud de la Prueba (Test accuracy): 0.8241999745368958\n"
     ]
    }
   ],
   "source": [
    "perdida, exactitud = modelo.evaluate(X_test, y_test,\n",
    "                            batch_size = 64,\n",
    "                            verbose = 2)\n",
    "print('Pérdida de la Prueba:', perdida)\n",
    "print('Exactitud de la Prueba (Test accuracy):', exactitud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"max\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m  4/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.4284 - auc: 0.3311 - loss: 0.7474 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757572907.767101    8772 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.7758 - auc: 0.8602 - loss: 0.4659 - val_accuracy: 0.8382 - val_auc: 0.9212 - val_loss: 0.3598 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9046 - auc: 0.9634 - loss: 0.2432 - val_accuracy: 0.8270 - val_auc: 0.9199 - val_loss: 0.4340 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9542 - auc: 0.9876 - loss: 0.1348 - val_accuracy: 0.8272 - val_auc: 0.9069 - val_loss: 0.4741 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9750 - auc: 0.9957 - loss: 0.0744 - val_accuracy: 0.8196 - val_auc: 0.8955 - val_loss: 0.5840 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9858 - auc: 0.9983 - loss: 0.0422 - val_accuracy: 0.8210 - val_auc: 0.8844 - val_loss: 0.8399 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9901 - auc: 0.9988 - loss: 0.0328 - val_accuracy: 0.8210 - val_auc: 0.8794 - val_loss: 0.9362 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9930 - auc: 0.9992 - loss: 0.0236 - val_accuracy: 0.8150 - val_auc: 0.8778 - val_loss: 0.8892 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.9947 - auc: 0.9996 - loss: 0.0165 - val_accuracy: 0.8154 - val_auc: 0.8743 - val_loss: 0.9440 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9973 - auc: 0.9997 - loss: 0.0103 - val_accuracy: 0.8058 - val_auc: 0.8551 - val_loss: 1.3115 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.9944 - auc: 0.9994 - loss: 0.0174 - val_accuracy: 0.8062 - val_auc: 0.8712 - val_loss: 0.9750 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    }
   ],
   "source": [
    "history_basico = modelo_basico.fit(\n",
    "    [X_train, train_features],\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_basico.save(\"Analisis_Sentimiento2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 - 2s - 4ms/step - accuracy: 0.8010 - auc: 0.8481 - loss: 1.3265\n",
      "Pérdida de la Prueba: 1.3264585733413696\n",
      "Exactitud de la Prueba (Test accuracy): 0.80103999376297\n",
      "AUC: 0.8481361269950867\n"
     ]
    }
   ],
   "source": [
    "perdida, AUC, exactitud = modelo_basico.evaluate([X_test, test_features], y_test,\n",
    "                            batch_size = 64,\n",
    "                            verbose = 2)\n",
    "print('Pérdida de la Prueba:', perdida)\n",
    "print('Exactitud de la Prueba (Test accuracy):', exactitud)\n",
    "print('AUC:', AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo mejorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "1PXwh2Tbgos7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 835ms/step - accuracy: 0.7298 - auc: 0.8142 - loss: 0.5414 - val_accuracy: 0.8284 - val_auc: 0.9153 - val_loss: 0.3870 - learning_rate: 3.0000e-04\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 01:29:55.062262: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 51200000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 876ms/step - accuracy: 0.8816 - auc: 0.9465 - loss: 0.3099 - val_accuracy: 0.8380 - val_auc: 0.9240 - val_loss: 0.3673 - learning_rate: 3.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 930ms/step - accuracy: 0.9421 - auc: 0.9814 - loss: 0.1817 - val_accuracy: 0.8460 - val_auc: 0.9248 - val_loss: 0.4053 - learning_rate: 1.5000e-04\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 01:39:20.895704: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 51200000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "history_avanzado = modelo_avanzado.fit(\n",
    "    [X_train_, train_features],\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 01:39:44.393003: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 51200000 exceeds 10% of free system memory.\n",
      "2025-09-11 01:39:44.638717: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 51200000 exceeds 10% of free system memory.\n",
      "2025-09-11 01:39:44.714026: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 51200000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "modelo_avanzado.save(\"Analisis_Sentimiento3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 - 45s - 116ms/step - accuracy: 0.8362 - auc: 0.9156 - loss: 0.3859\n",
      "Pérdida de la Prueba: 0.3858749270439148\n",
      "Exactitud de la Prueba (Test accuracy): 0.8361600041389465\n",
      "AUC: 0.9155688285827637\n"
     ]
    }
   ],
   "source": [
    "perdida, AUC, exactitud = modelo_avanzado.evaluate([X_test_, test_features], y_test,\n",
    "                            batch_size = 64,\n",
    "                            verbose = 2)\n",
    "print('Pérdida de la Prueba:', perdida)\n",
    "print('Exactitud de la Prueba (Test accuracy):', exactitud)\n",
    "print('AUC:', AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informe"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
